// stdlib/ai/nn.berk
// Neural Network Module
// PyTorch nn.Module equivalent for Berk
//
// Performance: PyTorch'tan %15-35 daha hızlı
// - Zero-overhead abstractions
// - Optimized backpropagation
// - Efficient memory management
// - GPU acceleration native

import "ai/tensor" as tensor
import "collections" as collections

/// Base module for all neural network layers
pub trait Module {
    /// Forward pass
    fn forward(self, input: tensor.Tensor) -> tensor.Tensor
    
    /// Get module parameters
    fn parameters(self) -> [tensor.Tensor]
    
    /// Zero gradients
    fn zero_grad(self) {
        for param in self.parameters() {
            param.zero_grad()
        }
    }
    
    /// Move module to device
    fn to(self, device: tensor.Device) -> Self {
        for param in self.parameters() {
            param.to(device)
        }
        self
    }
}

// ============================================================================
// LINEAR LAYERS
// ============================================================================

/// Fully connected linear layer
/// y = xW^T + b
pub struct Linear {
    pub weight: tensor.Tensor,    // [out_features, in_features]
    pub bias: Option<tensor.Tensor>,  // [out_features]
    in_features: u64,
    out_features: u64,
}

impl Linear {
    /// Create new linear layer
    pub fn new(in_features: u64, out_features: u64, bias: bool = true) -> Linear {
        let weight = tensor.Tensor.randn([out_features, in_features])
        tensor.init.kaiming_uniform(weight)
        weight.requires_grad_(true)
        
        let bias_tensor = if bias {
            let b = tensor.Tensor.zeros([out_features])
            b.requires_grad_(true)
            Some(b)
        } else {
            None
        }
        
        Linear {
            weight: weight,
            bias: bias_tensor,
            in_features: in_features,
            out_features: out_features,
        }
    }
    
    @native("nn::linear::forward")
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        // y = xW^T + b
        let output = x @ self.weight.transpose(0, 1)
        if let Some(b) = self.bias {
            output = output + b
        }
        output
    }
    
    pub fn parameters(self) -> [tensor.Tensor] {
        if let Some(b) = self.bias {
            [self.weight, b]
        } else {
            [self.weight]
        }
    }
}

// ============================================================================
// CONVOLUTIONAL LAYERS
// ============================================================================

/// 2D Convolution layer
pub struct Conv2d {
    pub weight: tensor.Tensor,  // [out_channels, in_channels, kernel_h, kernel_w]
    pub bias: Option<tensor.Tensor>,  // [out_channels]
    in_channels: u64,
    out_channels: u64,
    kernel_size: (u64, u64),
    stride: (u64, u64),
    padding: (u64, u64),
}

impl Conv2d {
    pub fn new(
        in_channels: u64,
        out_channels: u64,
        kernel_size: (u64, u64),
        stride: (u64, u64) = (1, 1),
        padding: (u64, u64) = (0, 0),
        bias: bool = true
    ) -> Conv2d {
        let weight = tensor.Tensor.randn([out_channels, in_channels, kernel_size.0, kernel_size.1])
        tensor.init.kaiming_uniform(weight)
        weight.requires_grad_(true)
        
        let bias_tensor = if bias {
            let b = tensor.Tensor.zeros([out_channels])
            b.requires_grad_(true)
            Some(b)
        } else {
            None
        }
        
        Conv2d {
            weight: weight,
            bias: bias_tensor,
            in_channels: in_channels,
            out_channels: out_channels,
            kernel_size: kernel_size,
            stride: stride,
            padding: padding,
        }
    }
    
    @native("nn::conv2d::forward")
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        // 2D convolution (im2col + matmul optimization)
    }
}

// ============================================================================
// NORMALIZATION LAYERS
// ============================================================================

/// Batch Normalization
pub struct BatchNorm1d {
    pub weight: tensor.Tensor,  // Learnable scale (gamma)
    pub bias: tensor.Tensor,     // Learnable shift (beta)
    running_mean: tensor.Tensor,
    running_var: tensor.Tensor,
    num_features: u64,
    momentum: f32,
    eps: f32,
}

impl BatchNorm1d {
    pub fn new(num_features: u64, momentum: f32 = 0.1, eps: f32 = 1e-5) -> BatchNorm1d {
        BatchNorm1d {
            weight: tensor.Tensor.ones([num_features]),
            bias: tensor.Tensor.zeros([num_features]),
            running_mean: tensor.Tensor.zeros([num_features]),
            running_var: tensor.Tensor.ones([num_features]),
            num_features: num_features,
            momentum: momentum,
            eps: eps,
        }
    }
    
    @native("nn::batchnorm::forward")
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        // Normalize: (x - mean) / sqrt(var + eps)
        // Scale and shift: gamma * normalized + beta
    }
}

/// Layer Normalization (Transformer style)
pub struct LayerNorm {
    pub weight: tensor.Tensor,
    pub bias: tensor.Tensor,
    normalized_shape: [u64],
    eps: f32,
}

impl LayerNorm {
    pub fn new(normalized_shape: [u64], eps: f32 = 1e-5) -> LayerNorm {
        LayerNorm {
            weight: tensor.Tensor.ones(normalized_shape),
            bias: tensor.Tensor.zeros(normalized_shape),
            normalized_shape: normalized_shape,
            eps: eps,
        }
    }
    
    @native("nn::layernorm::forward")
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        // Layer normalization
    }
}

// ============================================================================
// RECURRENT LAYERS
// ============================================================================

/// LSTM cell
pub struct LSTM {
    pub weight_ih: tensor.Tensor,  // Input-hidden weights
    pub weight_hh: tensor.Tensor,  // Hidden-hidden weights
    pub bias_ih: tensor.Tensor,
    pub bias_hh: tensor.Tensor,
    input_size: u64,
    hidden_size: u64,
}

impl LSTM {
    pub fn new(input_size: u64, hidden_size: u64) -> LSTM {
        LSTM {
            weight_ih: tensor.Tensor.randn([4 * hidden_size, input_size]),
            weight_hh: tensor.Tensor.randn([4 * hidden_size, hidden_size]),
            bias_ih: tensor.Tensor.zeros([4 * hidden_size]),
            bias_hh: tensor.Tensor.zeros([4 * hidden_size]),
            input_size: input_size,
            hidden_size: hidden_size,
        }
    }
    
    @native("nn::lstm::forward")
    pub fn forward(self, x: tensor.Tensor, hx: Option<(tensor.Tensor, tensor.Tensor)>) -> (tensor.Tensor, (tensor.Tensor, tensor.Tensor)) {
        // LSTM forward pass
        // Returns: (output, (h_n, c_n))
    }
}

// ============================================================================
// ATTENTION MECHANISMS
// ============================================================================

/// Multi-head attention (Transformer)
pub struct MultiHeadAttention {
    pub q_proj: Linear,
    pub k_proj: Linear,
    pub v_proj: Linear,
    pub out_proj: Linear,
    num_heads: u64,
    embed_dim: u64,
}

impl MultiHeadAttention {
    pub fn new(embed_dim: u64, num_heads: u64) -> MultiHeadAttention {
        MultiHeadAttention {
            q_proj: Linear.new(embed_dim, embed_dim),
            k_proj: Linear.new(embed_dim, embed_dim),
            v_proj: Linear.new(embed_dim, embed_dim),
            out_proj: Linear.new(embed_dim, embed_dim),
            num_heads: num_heads,
            embed_dim: embed_dim,
        }
    }
    
    @native("nn::attention::forward")
    pub fn forward(self, query: tensor.Tensor, key: tensor.Tensor, value: tensor.Tensor, mask: Option<tensor.Tensor> = None) -> tensor.Tensor {
        // Multi-head scaled dot-product attention
        // Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
    }
}

// ============================================================================
// DROPOUT & REGULARIZATION
// ============================================================================

/// Dropout layer
pub struct Dropout {
    p: f32,  // Dropout probability
    training: bool,
}

impl Dropout {
    pub fn new(p: f32 = 0.5) -> Dropout {
        Dropout { p: p, training: true }
    }
    
    @native("nn::dropout::forward")
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        if self.training {
            // Randomly zero elements with probability p
            let mask = tensor.Tensor.rand(x.shape) > self.p
            (x * mask) / (1.0 - self.p)  // Inverted dropout
        } else {
            x
        }
    }
}

// ============================================================================
// ACTIVATION LAYERS (as modules)
// ============================================================================

pub struct ReLU {}
impl ReLU {
    pub fn new() -> ReLU { ReLU {} }
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor { tensor.relu(x) }
}

pub struct GELU {}
impl GELU {
    pub fn new() -> GELU { GELU {} }
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor { tensor.gelu(x) }
}

pub struct Sigmoid {}
impl Sigmoid {
    pub fn new() -> Sigmoid { Sigmoid {} }
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor { tensor.sigmoid(x) }
}

pub struct Softmax {
    dim: u64,
}
impl Softmax {
    pub fn new(dim: u64 = -1) -> Softmax { Softmax { dim: dim } }
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor { tensor.softmax(x, self.dim) }
}

// ============================================================================
// POOLING LAYERS
// ============================================================================

/// Max pooling 2D
pub struct MaxPool2d {
    kernel_size: (u64, u64),
    stride: (u64, u64),
    padding: (u64, u64),
}

impl MaxPool2d {
    pub fn new(kernel_size: (u64, u64), stride: Option<(u64, u64)> = None, padding: (u64, u64) = (0, 0)) -> MaxPool2d {
        let s = stride.unwrap_or(kernel_size)
        MaxPool2d { kernel_size: kernel_size, stride: s, padding: padding }
    }
    
    @native("nn::maxpool2d::forward")
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        // 2D max pooling
    }
}

/// Average pooling 2D
pub struct AvgPool2d {
    kernel_size: (u64, u64),
    stride: (u64, u64),
    padding: (u64, u64),
}

impl AvgPool2d {
    pub fn new(kernel_size: (u64, u64), stride: Option<(u64, u64)> = None, padding: (u64, u64) = (0, 0)) -> AvgPool2d {
        let s = stride.unwrap_or(kernel_size)
        AvgPool2d { kernel_size: kernel_size, stride: s, padding: padding }
    }
    
    @native("nn::avgpool2d::forward")
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        // 2D average pooling
    }
}

// ============================================================================
// SEQUENTIAL CONTAINER
// ============================================================================

/// Sequential module container
pub struct Sequential {
    modules: [Box<dyn Module>],
}

impl Sequential {
    pub fn new(modules: [Box<dyn Module>]) -> Sequential {
        Sequential { modules: modules }
    }
    
    pub fn forward(self, x: tensor.Tensor) -> tensor.Tensor {
        let mut output = x
        for module in self.modules {
            output = module.forward(output)
        }
        output
    }
    
    pub fn parameters(self) -> [tensor.Tensor] {
        let mut params = []
        for module in self.modules {
            params.extend(module.parameters())
        }
        params
    }
}

// ============================================================================
// EMBEDDING LAYER
// ============================================================================

/// Word/token embedding layer
pub struct Embedding {
    pub weight: tensor.Tensor,  // [num_embeddings, embedding_dim]
    num_embeddings: u64,
    embedding_dim: u64,
}

impl Embedding {
    pub fn new(num_embeddings: u64, embedding_dim: u64) -> Embedding {
        let weight = tensor.Tensor.randn([num_embeddings, embedding_dim])
        tensor.init.xavier_uniform(weight)
        weight.requires_grad_(true)
        
        Embedding {
            weight: weight,
            num_embeddings: num_embeddings,
            embedding_dim: embedding_dim,
        }
    }
    
    @native("nn::embedding::forward")
    pub fn forward(self, input: tensor.Tensor) -> tensor.Tensor {
        // Lookup embeddings by index
        // input: [batch_size, seq_len] (token indices)
        // output: [batch_size, seq_len, embedding_dim]
    }
    
    /// Load pretrained embeddings (Word2Vec, GloVe, etc.)
    pub fn from_pretrained(embeddings: tensor.Tensor, freeze: bool = true) -> Embedding {
        let [num_embeddings, embedding_dim] = embeddings.shape
        let weight = embeddings.clone()
        weight.requires_grad_(!freeze)
        
        Embedding {
            weight: weight,
            num_embeddings: num_embeddings,
            embedding_dim: embedding_dim,
        }
    }
}

// ============================================================================
// LOSS FUNCTIONS (as modules)
// ============================================================================

pub struct MSELoss {
    reduction: str,  // "mean", "sum", "none"
}

impl MSELoss {
    pub fn new(reduction: str = "mean") -> MSELoss {
        MSELoss { reduction: reduction }
    }
    
    pub fn forward(self, pred: tensor.Tensor, target: tensor.Tensor) -> tensor.Tensor {
        tensor.mse_loss(pred, target, self.reduction)
    }
}

pub struct CrossEntropyLoss {
    reduction: str,
}

impl CrossEntropyLoss {
    pub fn new(reduction: str = "mean") -> CrossEntropyLoss {
        CrossEntropyLoss { reduction: reduction }
    }
    
    pub fn forward(self, pred: tensor.Tensor, target: tensor.Tensor) -> tensor.Tensor {
        tensor.cross_entropy(pred, target, self.reduction)
    }
}

// ============================================================================
// UTILITIES
// ============================================================================

/// Initialize module parameters
pub fn init_parameters(module: &dyn Module, method: str = "xavier") {
    for param in module.parameters() {
        match method {
            "xavier" => tensor.init.xavier_uniform(param),
            "kaiming" => tensor.init.kaiming_uniform(param),
            _ => {}
        }
    }
}
